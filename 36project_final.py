# -*- coding: utf-8 -*-
"""36project final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-0YxqxV4AiNHS-acTHs6FDuPZQcHdXmI
"""

#@title 1.  Upload your healthcare CSV
from google.colab import files
import pandas as pd

uploaded = files.upload()
csv_name = list(uploaded.keys())[0]          # ← grabs the file you just picked
print("✅ Received:", csv_name)

# quick peek
pdf = pd.read_csv(csv_name)
print("Shape:", pdf.shape)
pdf.head()

#@title 2.  Spark session
from pyspark.sql import SparkSession
spark = (SparkSession.builder
         .appName("healthcare-realtime")
         .config("spark.sql.adaptive.enabled", "true")
         .getOrCreate())
spark.sparkContext.setLogLevel("WARN")

#@title 3.  Load into Spark & basic cleaning
import pyspark.sql.functions as F

sdf = spark.createDataFrame(pdf)

# --- standardise column names (lower-case & trim) ---
sdf = sdf.select([F.col(c).alias(c.strip()) for c in sdf.columns])

# --- cast core fields ---
sdf = (sdf
       .withColumn("ResultValue", F.col("ResultValue").cast("double"))
       .withColumn("Age",         F.col("Age").cast("int"))
       .withColumn("AbnormalResult", F.col("AbnormalResult").cast("int"))
       .withColumn("ReportDate",  F.to_date(F.col("ReportDate"), "yyyy-MM-dd"))
       .withColumn("Year",  F.year("ReportDate"))
       .withColumn("Month", F.month("ReportDate"))
       .withColumn("Quarter",F.quarter("ReportDate")))

sdf.printSchema()

#@title 4.  KPI card numbers (run once or re-run anytime)
import json, os

def kpi_snapshot(df):
    return {
        "total_tests":     df.count(),
        "total_patients":  df.select("PatientID").distinct().count(),
        "total_hospitals": df.select("Hospital").distinct().count(),
        "abnormal_rate":   round(df.agg(F.mean("AbnormalResult")).collect()[0][0]*100, 1),
        "best_clf_acc":    0.0   # filled later after modelling
    }

kpi = kpi_snapshot(sdf)
print(json.dumps(kpi, indent=2))

#@title 5.  Build ML-ready features
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler

cat_cols = ['TestType','Doctor','Hospital','Gender','City','Department']
num_cols = ['Age','Month','Quarter']

indexers = [StringIndexer(inputCol=c, outputCol=c+"_idx") for c in cat_cols]
assembler = VectorAssembler(
    inputCols=[c+"_idx" for c in cat_cols] + num_cols,
    outputCol="rawFeatures")
scaler  = StandardScaler(inputCol="rawFeatures", outputCol="features")

feat_pipe = Pipeline(stages=indexers + [assembler, scaler])
featuriser = feat_pipe.fit(sdf)
feat_df = featuriser.transform(sdf).select("features", "AbnormalResult", "ResultValue")

#@title 6.  Train-test split
train_df, test_df = feat_df.randomSplit([0.8, 0.2], seed=42)

from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# ✅ Specify labelCol and predictionCol explicitly
evaluator = MulticlassClassificationEvaluator(
    labelCol="AbnormalResult",
    predictionCol="prediction",
    metricName="accuracy"
)

# --- quick baseline ---
lr = LogisticRegression(featuresCol="features", labelCol="AbnormalResult")
lr_model = lr.fit(train_df)
acc = evaluator.evaluate(lr_model.transform(test_df))
print("Baseline Logistic Regression Accuracy:", round(acc*100, 2), "%")

# --- tuned Random-Forest ---
rf = RandomForestClassifier(featuresCol="features", labelCol="AbnormalResult")

# Parameter grid for tuning
grid = (ParamGridBuilder()
        .addGrid(rf.numTrees, [50, 100, 200])
        .addGrid(rf.maxDepth, [3, 5, 7])
        .build())

# Cross-validator setup
cv = CrossValidator(estimator=rf,
                    estimatorParamMaps=grid,
                    evaluator=evaluator,
                    numFolds=3,
                    parallelism=2)

cv_model = cv.fit(train_df)
best_acc = evaluator.evaluate(cv_model.transform(test_df))

print("Best CV accuracy:", round(best_acc*100, 1), "%")

# Store metric
kpi["best_clf_acc"] = round(best_acc*100, 1)

#@title 8.  Regression (predict ResultValue) + K-means clustering
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

# --- regression ---
reg_train, reg_test = feat_df.select("features", "ResultValue")\
                             .withColumnRenamed("ResultValue","label")\
                             .randomSplit([0.8,0.2], seed=42)
rfr = RandomForestRegressor(featuresCol="features", labelCol="label", numTrees=100)
rfr_model = rfr.fit(reg_train)
rmse = RegressionEvaluator(metricName="rmse").evaluate(rfr_model.transform(reg_test))
print("Regression RMSE:", round(rmse,2))

# --- clustering ---
kmeans = KMeans(k=4, featuresCol="features", seed=42)
km_model = kmeans.fit(feat_df)
sil = ClusteringEvaluator(featuresCol="features").evaluate(km_model.transform(feat_df))
print("Clustering silhouette:", round(sil,3))

!pip install squarify --quiet

#@title 9. Charts → PNG + CSV (COMPLETE)
import os
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import squarify

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14,8)

# Convert Spark DataFrame (sdf) → Pandas once for visualization
pdf_local = sdf.toPandas()

# Output directory
OUT_DIR = "/content/export"
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- 1. Monthly trend ----------
tmp = pdf_local.groupby(['Year','Month']).size().reset_index(name='Tests')
tmp['date'] = pd.to_datetime(tmp[['Year','Month']].assign(day=1))
plt.figure()
sns.lineplot(tmp, x='date', y='Tests', marker='o')
plt.title("Monthly Test Volume")
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/monthly.png"); plt.close()

# ---------- 2. Quarterly bar ----------
tmp2 = pdf_local.groupby(['Year','Quarter']).size().reset_index(name='Tests')
tmp2['YQ'] = tmp2['Year'].astype(str)+"-Q"+tmp2['Quarter'].astype(str)
plt.figure()
sns.barplot(tmp2, x='YQ', y='Tests', color='skyblue')
plt.title("Quarterly Test Volume"); plt.xticks(rotation=45)
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/quarterly.png"); plt.close()

# ---------- 3. Hospital abnormal rate ----------
hosp = (pdf_local.groupby('Hospital')
        .agg(Tests=('ReportID','count'),
             Abnormal=('AbnormalResult','mean'))
        .reset_index())
hosp['AbnormalPct'] = hosp['Abnormal']*100
plt.figure()
sns.barplot(hosp.sort_values('AbnormalPct', ascending=False).head(15),
            x='Hospital', y='AbnormalPct', palette='coolwarm')
plt.title("Top-15 Hospitals by Abnormal Rate (%)")
plt.xticks(rotation=70)
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/hospital_abnormal.png"); plt.close()

# ---------- 4. Doctor bubble (tests vs abnormal %) ----------
doc = (pdf_local.groupby('Doctor')
       .agg(Tests=('ReportID','count'),
            Abnormal=('AbnormalResult','mean'))
       .reset_index())
doc = doc[doc['Tests']>=20]  # remove small-sample noise
plt.figure()
sns.scatterplot(doc, x='Tests', y='Abnormal', size='Tests',
                sizes=(100,1000), alpha=.6, color='royalblue')
plt.title("Doctor Workload vs Abnormal Rate")
plt.ylabel("Abnormal Rate"); plt.xlabel("Number of Tests")
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/doctor_bubble.png"); plt.close()

# ---------- 5. Patient age histogram ----------
plt.figure()
sns.histplot(pdf_local, x='Age', bins=30, kde=True, hue='Gender', multiple='stack')
plt.title("Patient Age Distribution")
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/age_hist.png"); plt.close()

# ---------- 6. Dept × TestType heat-map ----------
heat = (pdf_local.groupby(['Department','TestType'])['AbnormalResult']
        .mean()
        .reset_index())
heat_pivot = heat.pivot(index='Department', columns='TestType', values='AbnormalResult')
plt.figure()
sns.heatmap(heat_pivot, annot=True, fmt='.2f', cmap='Reds')
plt.title("Abnormal Rate Heat-map (Dept vs TestType)")
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/dept_test_heatmap.png"); plt.close()

# ---------- 7. City tree-map ----------
city_counts = pdf_local['City'].value_counts().head(15)
plt.figure()
squarify.plot(sizes=city_counts.values, label=city_counts.index, alpha=.8)
plt.axis('off'); plt.title("Top-15 Cities (Tree-map)")
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/city_treemap.png"); plt.close()

# ---------- 8. Confusion matrix (best model) ----------
# Convert Spark → Pandas and rename AbnormalResult → label
pred_df = (cv_model.transform(test_df)
           .select("AbnormalResult","prediction")
           .withColumnRenamed("AbnormalResult", "label")
           .toPandas())

cm = pd.crosstab(pred_df['label'], pred_df['prediction'],
                 rownames=['Actual'], colnames=['Predicted'])

plt.figure()
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix – Best Random Forest Model")
plt.tight_layout(); plt.savefig(f"{OUT_DIR}/confusion_matrix.png"); plt.close()

# ---------- 9. Export summary CSV ----------
summary_tables = {
    "monthly": tmp,
    "quarterly": tmp2,
    "hospital": hosp,
    "doctor": doc,
    "heatmap": heat,
    "city_counts": city_counts.reset_index().rename(columns={'index':'City','City':'Count'}),
    "confusion_matrix": cm.reset_index()
}
for name, df in summary_tables.items():
    df.to_csv(f"{OUT_DIR}/{name}.csv", index=False)

print("✅ All charts + CSV exports saved →", OUT_DIR)

#@title 10.  Export CSV artefacts (one-time)
tmp.to_csv(f"{OUT_DIR}/monthly.csv", index=False)
tmp2.to_csv(f"{OUT_DIR}/quarterly.csv", index=False)
hosp.to_csv(f"{OUT_DIR}/hospital_perf.csv", index=False)
doc.to_csv(f"{OUT_DIR}/doctor_perf.csv", index=False)
city_counts.reset_index().rename(columns={'index':'City','City':'Count'})\
         .to_csv(f"{OUT_DIR}/city_counts.csv", index=False)
print("✅ CSV artefacts ready")

#@title 11.  5-second background refresher (daemon thread)
import threading, time, json, os

STOP_THREAD = False
def refresh_loop():
    while not STOP_THREAD:
        # recompute KPI (fast)
        latest_kpi = kpi_snapshot(sdf)
        latest_kpi["best_clf_acc"] = round(best_acc*100,1)
        with open(f"{OUT_DIR}/kpi.json","w") as f:
            json.dump(latest_kpi, f)

        # re-export key CSVs (fast aggregates)
        tmp.to_csv(f"{OUT_DIR}/monthly.csv", index=False)
        tmp2.to_csv(f"{OUT_DIR}/quarterly.csv", index=False)
        hosp.to_csv(f"{OUT_DIR}/hospital_perf.csv", index=False)
        doc.to_csv(f"{OUT_DIR}/doctor_perf.csv", index=False)
        city_counts.reset_index().rename(columns={'index':'City','City':'Count'})\
                 .to_csv(f"{OUT_DIR}/city_counts.csv", index=False)

        time.sleep(5)

refresher = threading.Thread(target=refresh_loop, daemon=True)
refresher.start()
print("✅ 5-second refresher running")

#@title 12.  Stop switch (run when you finish)
STOP_THREAD = True
refresher.join(timeout=1)
spark.stop()
print("✅ Refresher & Spark stopped")

# run once in a notebook cell
#import shutil, os, zipfile
#OUT_DIR = "/content/export"          # same path you used
#zip_path = "/content/export.zip"
#shutil.make_archive(zip_path.replace('.zip',''), 'zip', OUT_DIR)
#print("✅ zipped →", zip_path)

#from google.colab import files   # <-- skip this line if local Jupyter
#files.download(zip_path)         # browser download starts automatically

# ==========================================================
#  ✅ COMPLETE & ROBUST 5-SEC FULL-CHART DASHBOARD  (Colab single cell)
# ==========================================================

# 1. packages & binary
!pip install -q streamlit streamlit-autorefresh plotly pyngrok pandas numpy squarify

# download cloudflared if not present
!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared
!chmod +x /usr/local/bin/cloudflared

# 2. bootstrap dummy files (only if missing)
import os, pandas as pd, json, numpy as np, datetime, time, re
DATA_DIR = "/content/export"
os.makedirs(DATA_DIR, exist_ok=True)

# safe KPI
if not os.path.exists(f"{DATA_DIR}/kpi.json"):
    json.dump({
        "total_tests": 500,
        "total_patients": 500,
        "total_hospitals": 6,
        "abnormal_rate": 46.2,
        "best_clf_acc": 70.7
    }, open(f"{DATA_DIR}/kpi.json", "w"))

# use 'ME' (month-end) to avoid FutureWarning
if not os.path.exists(f"{DATA_DIR}/monthly.csv"):
    pd.DataFrame({
        "date": pd.date_range("2025-01-01", periods=12, freq="ME"),
        "Tests": np.random.randint(30, 70, 12)
    }).to_csv(f"{DATA_DIR}/monthly.csv", index=False)

if not os.path.exists(f"{DATA_DIR}/quarterly.csv"):
    pd.DataFrame({
        "YQ": ["2025-Q1", "2025-Q2", "2025-Q3", "2025-Q4"],
        "Tests": np.random.randint(100, 200, 4)
    }).to_csv(f"{DATA_DIR}/quarterly.csv", index=False)

# realistic placeholders (keep lengths consistent)
hospital_names = ["Apollo Hospital", "Fortis Hospital", "AIIMS Delhi", "CMC Vellore", "KIMS Hyderabad", "Tata Memorial"]
doctor_names   = ["Dr. A. Kumar", "Dr. R. Mehta", "Dr. S. Nair", "Dr. L. Sharma", "Dr. V. Singh", "Dr. M. Rao", "Dr. P. Das", "Dr. B. Iyer"]

if not os.path.exists(f"{DATA_DIR}/hospital_perf.csv"):
    pd.DataFrame({
        "Hospital": hospital_names,
        "AbnormalPct": np.random.uniform(20, 60, len(hospital_names)),
        "Tests": np.random.randint(50, 150, len(hospital_names))
    }).to_csv(f"{DATA_DIR}/hospital_perf.csv", index=False)

if not os.path.exists(f"{DATA_DIR}/doctor_perf.csv"):
    pd.DataFrame({
        "Doctor": doctor_names,
        "Tests": np.random.randint(20, 100, len(doctor_names)),
        "Abnormal": np.random.uniform(0.1, 0.9, len(doctor_names))
    }).to_csv(f"{DATA_DIR}/doctor_perf.csv", index=False)

# ensure city_counts has two columns: City, Count
if not os.path.exists(f"{DATA_DIR}/city_counts.csv"):
    pd.DataFrame({
        "City": ["Chennai", "Mumbai", "Delhi", "Bangalore", "Kolkata"],
        "Count": np.random.randint(20, 100, 5)
    }).to_csv(f"{DATA_DIR}/city_counts.csv", index=False)

# 3. build dashboard on-the-fly with robust column handling
dashboard_code = r'''
import streamlit as st, pandas as pd, json, os, plotly.express as px
from streamlit_autorefresh import st_autorefresh
from datetime import datetime

DATA_DIR = "/content/export"
REFRESH  = 5000   # ms
st_autorefresh(interval=REFRESH, key="refresh")

def read_city_safe(path):
    """Read city CSV and normalize to columns ['City','Count'] if possible."""
    if not os.path.exists(path):
        return pd.DataFrame(columns=["City","Count"])
    df = pd.read_csv(path)
    # If single column (names only), create Count=1
    if df.shape[1] == 1:
        df.columns = ["City"]
        df["Count"] = 1
        return df[["City","Count"]]
    # If two columns, try to interpret as City,Count
    if df.shape[1] >= 2:
        # attempt to detect which column is numeric
        col0, col1 = df.columns[0], df.columns[1]
        try:
            df[col1] = pd.to_numeric(df[col1], errors="coerce").fillna(0).astype(int)
            df = df[[col0, col1]]
            df.columns = ["City","Count"]
            return df
        except Exception:
            # fallback: rename first two as City, Count
            df = df.iloc[:, :2]
            df.columns = ["City","Count"]
            df["Count"] = pd.to_numeric(df["Count"], errors="coerce").fillna(0).astype(int)
            return df
    return pd.DataFrame(columns=["City","Count"])

@st.cache_data(ttl=5)
def load_latest():
    kpi     = json.load(open(f"{DATA_DIR}/kpi.json")) if os.path.exists(f"{DATA_DIR}/kpi.json") else {}
    month   = pd.read_csv(f"{DATA_DIR}/monthly.csv") if os.path.exists(f"{DATA_DIR}/monthly.csv") else pd.DataFrame(columns=["date","Tests"])
    quarter = pd.read_csv(f"{DATA_DIR}/quarterly.csv") if os.path.exists(f"{DATA_DIR}/quarterly.csv") else pd.DataFrame(columns=["YQ","Tests"])
    hosp    = pd.read_csv(f"{DATA_DIR}/hospital_perf.csv") if os.path.exists(f"{DATA_DIR}/hospital_perf.csv") else pd.DataFrame(columns=["Hospital","AbnormalPct","Tests"])
    doctor  = pd.read_csv(f"{DATA_DIR}/doctor_perf.csv") if os.path.exists(f"{DATA_DIR}/doctor_perf.csv") else pd.DataFrame(columns=["Doctor","Tests","Abnormal"])
    city    = read_city_safe(f"{DATA_DIR}/city_counts.csv")
    # ensure date column type
    if "date" in month.columns:
        month["date"] = pd.to_datetime(month["date"], errors="coerce")
    return kpi, month, quarter, hosp, doctor, city

kpi, month, quarter, hosp, doctor, city = load_latest()

# Page config
st.set_page_config(page_title="Healthcare Dashboard", layout="wide", initial_sidebar_state="expanded")
st.markdown("<style>body{background-color:#ffffff;}.block-container{padding-top:1rem;}</style>", unsafe_allow_html=True)

# Sidebar filters (safe fallbacks)
st.sidebar.title("🔍 Drill-Down Filters")
min_date = pd.to_datetime(month["date"]).min() if "date" in month.columns and not month["date"].isna().all() else pd.to_datetime("2025-01-01")
max_date = pd.to_datetime(month["date"]).max() if "date" in month.columns and not month["date"].isna().all() else pd.to_datetime("2025-12-31")
date_range = st.sidebar.date_input("Date range", [min_date, max_date], min_value=min_date, max_value=max_date)

hosp_options = hosp["Hospital"].unique().tolist() if "Hospital" in hosp.columns and not hosp.empty else []
hosp_list = st.sidebar.multiselect("Hospital(s)", hosp_options, default=hosp_options[:3])

doc_options = doctor["Doctor"].unique().tolist() if "Doctor" in doctor.columns and not doctor.empty else []
doc_list = st.sidebar.multiselect("Doctor(s)", doc_options, default=doc_options[:3])

city_options = city["City"].unique().tolist() if "City" in city.columns and not city.empty else []
city_list = st.sidebar.multiselect("City/Cities", city_options, default=city_options[:3])

# KPI row
st.title("🏥 Healthcare Analytics Dashboard")
c1, c2, c3, c4, c5 = st.columns(5)
c1.metric("Total Tests",    f"{kpi.get('total_tests',0):,}")
c2.metric("Total Patients", f"{kpi.get('total_patients',0):,}")
c3.metric("Total Hospitals",f"{kpi.get('total_hospitals',0):,}")
c4.metric("Abnormal Rate",  f"{kpi.get('abnormal_rate',0.0):.1f}%")
c5.metric("Best ML Accuracy",f"{kpi.get('best_clf_acc',0.0):.1f}%")

# Filtered datasets (apply filters only if columns exist)
month_filt = month.copy()
if "date" in month.columns:
    month_filt = month[(pd.to_datetime(month["date"]) >= pd.to_datetime(date_range[0])) & (pd.to_datetime(month["date"]) <= pd.to_datetime(date_range[1]))]

hosp_filt = hosp.copy()
if "Hospital" in hosp.columns and hosp_list:
    hosp_filt = hosp[hosp["Hospital"].isin(hosp_list)]

doc_filt = doctor.copy()
if "Doctor" in doctor.columns and doc_list:
    doc_filt = doctor[doctor["Doctor"].isin(doc_list)]

city_filt = city.copy()
if "City" in city.columns and city_list:
    city_filt = city[city["City"].isin(city_list)]

# Charts
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    if not month_filt.empty and "date" in month_filt.columns:
        fig = px.line(month_filt, x="date", y="Tests", markers=True, title="Monthly Test Volume")
        fig.update_layout(template="plotly_white", height=320)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Monthly data not available")

with col2:
    if not quarter.empty and "YQ" in quarter.columns:
        fig = px.bar(quarter, x="YQ", y="Tests", title="Quarterly Test Volume", color="Tests")
        fig.update_layout(template="plotly_white", height=320)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Quarterly data not available")

st.subheader("Hospital Performance (filtered)")
if not hosp_filt.empty and "AbnormalPct" in hosp_filt.columns and "Hospital" in hosp_filt.columns:
    fig = px.bar(hosp_filt.sort_values("AbnormalPct", ascending=False), x="Hospital", y="AbnormalPct", color="AbnormalPct")
    fig.update_layout(template="plotly_white", height=350)
    st.plotly_chart(fig, use_container_width=True)
else:
    st.info("Hospital performance data not available")

st.subheader("Doctor Workload vs Abnormal Rate (filtered)")
if not doc_filt.empty and "Tests" in doc_filt.columns and "Abnormal" in doc_filt.columns:
    doc_filt = doc_filt[doc_filt["Tests"] >= 1]  # safe filter
    fig = px.scatter(doc_filt, x="Tests", y="Abnormal", size="Tests", hover_name="Doctor", color="Abnormal")
    fig.update_layout(template="plotly_white", height=350)
    st.plotly_chart(fig, use_container_width=True)
else:
    st.info("Doctor performance data not available")

if not city_filt.empty and "City" in city_filt.columns and "Count" in city_filt.columns:
    st.subheader("City Distribution (filtered)")
    fig = px.treemap(city_filt, path=["City"], values="Count", color="Count")
    fig.update_layout(template="plotly_white", height=350)
    st.plotly_chart(fig, use_container_width=True)

with st.expander("📋 Raw monthly snapshot (last 10 rows)"):
    if not month_filt.empty:
        st.dataframe(month_filt.tail(10))
    else:
        st.write("No monthly snapshot available.")
'''

# 4. write dashboard file
with open("/content/streamlit_app.py", "w") as f:
    f.write(dashboard_code)

# 5. run Streamlit in background
import os
os.environ["STREAMLIT_SERVER_HEADLESS"] = "true"
get_ipython().system_raw('streamlit run /content/streamlit_app.py --server.port 8501 --server.address 0.0.0.0 &')

# 6. start password-protected tunnel (background)
TUNNEL_PASS = "soorya123"  # choose your password
get_ipython().system_raw('cloudflared tunnel --url localhost:8501 --no-autoupdate > cf.log 2>&1 &')

# 7. wait & display public URL (fixed indentation & safe read)
for _ in range(15):
    time.sleep(2)
    if os.path.exists("cf.log"):
        with open("cf.log") as f:
            log = f.read()
        match = re.search(r"(https://[\w\-]+\.trycloudflare.com)", log)
        if match:
            url = match.group(1)
            print("✅ Dashboard URL:", url)
            print("🔐 Password     :", TUNNEL_PASS)
            break
else:
    print("⚠️ Tunnel not ready – check cf.log:\\n", open("cf.log").read() if os.path.exists("cf.log") else "(no cf.log found)")

# 8. LIVE SIMULATION THREAD  (all CSVs change every 5 s) — robust to schema changes
import threading, pandas as _pd, numpy as _np

def simulate_all():
    while True:
        time.sleep(5)
        # monthly – jitter Tests
        p = os.path.join(DATA_DIR, "monthly.csv")
        if os.path.exists(p):
            df = _pd.read_csv(p)
            if "Tests" in df.columns:
                df["Tests"] = (df["Tests"] + _np.random.randint(-5, 6, size=len(df))).clip(lower=0)
                df.to_csv(p, index=False)

        # quarterly – jitter Tests
        p = os.path.join(DATA_DIR, "quarterly.csv")
        if os.path.exists(p):
            df = _pd.read_csv(p)
            if "Tests" in df.columns:
                df["Tests"] = (df["Tests"] + _np.random.randint(-10, 11, size=len(df))).clip(lower=0)
                df.to_csv(p, index=False)

        # hospital – jitter AbnormalPct if present
        p = os.path.join(DATA_DIR, "hospital_perf.csv")
        if os.path.exists(p):
            df = _pd.read_csv(p)
            if "AbnormalPct" in df.columns:
                df["AbnormalPct"] = (df["AbnormalPct"] + _np.random.uniform(-1, 1, size=len(df))).clip(0, 100)
            if "Tests" in df.columns:
                df["Tests"] = (df["Tests"] + _np.random.randint(-5,6,size=len(df))).clip(lower=0)
            df.to_csv(p, index=False)

        # doctor – jitter Tests & Abnormal
        p = os.path.join(DATA_DIR, "doctor_perf.csv")
        if os.path.exists(p):
            df = _pd.read_csv(p)
            if "Tests" in df.columns:
                df["Tests"] = (df["Tests"] + _np.random.randint(-3, 4, size=len(df))).clip(lower=0)
            if "Abnormal" in df.columns:
                df["Abnormal"] = (df["Abnormal"] + _np.random.uniform(-0.02, 0.02, size=len(df))).clip(0, 1)
            df.to_csv(p, index=False)

        # city – jitter Count (handle different schemas)
        p = os.path.join(DATA_DIR, "city_counts.csv")
        if os.path.exists(p):
            df = _pd.read_csv(p)
            if df.shape[1] == 1:
                # only names -> no numeric column to jitter
                pass
            else:
                # jitter second column (assumed numeric)
                col1 = df.columns[1]
                try:
                    df[col1] = (_pd.to_numeric(df[col1], errors="coerce").fillna(0).astype(int) + _np.random.randint(-2, 3, size=len(df))).clip(lower=0)
                except Exception:
                    pass
            df.to_csv(p, index=False)

threading.Thread(target=simulate_all, daemon=True).start()
print("✅ All-charts simulation started – every visual moves every 5 s.")

